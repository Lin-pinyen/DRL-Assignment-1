import numpy as np
import random
import math
import os 
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque, namedtuple

# ä½¿ç”¨namedtupleæ¥å­˜å‚¨ç»éªŒï¼Œæé«˜ä»£ç å¯è¯»æ€§
Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))

class ReplayMemory:
    def __init__(self, capacity):
        self.memory = deque([], maxlen=capacity)
        
    def push(self, *args):
        """Save a transition"""
        self.memory.append(Transition(*args))
        
    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)
    
    def __len__(self):
        return len(self.memory)

class SimpleTaxiEnv():
    def __init__(self, grid_size=5, fuel_limit=200):
        self.grid_size = grid_size
        self.fuel_limit = fuel_limit
        self.current_fuel = fuel_limit
        self.passenger_picked_up = False
        self.stations = [(0, 0), (0, self.grid_size - 1),
                         (self.grid_size - 1, 0), (self.grid_size - 1, self.grid_size - 1)]
        self.passenger_loc = None
        self.obstacles = set()  # ç®€æ˜“ç‰ˆæ— éšœç¢ç‰©
        self.destination = None

        # ç»Ÿè®¡æ•°æ®
        self.successful_pickups = 0
        self.successful_dropoffs = 0
        self.action_history = []
        
        # è®°å½•æœ€è¿‘åˆ°è¾¾è¿‡çš„ä½ç½®ï¼Œç”¨äºæ£€æµ‹å¾ªç¯è¡Œä¸º
        self.recent_positions = []
        self.position_history_limit = 20
        
    def reset(self):
        """é‡ç½®ç¯å¢ƒï¼Œç¡®ä¿ Taxiã€ä¹˜å®¢ä¸ç›®çš„åœ°äº’ä¸é‡å """
        self.current_fuel = self.fuel_limit
        self.passenger_picked_up = False
        self.action_history = []
        self.recent_positions = []
        
        available_positions = [
            (x, y) for x in range(self.grid_size) for y in range(self.grid_size)
            if (x, y) not in self.stations and (x, y) not in self.obstacles
        ]
        self.taxi_pos = random.choice(available_positions)
        
        self.passenger_loc = random.choice(self.stations)
        possible_destinations = [s for s in self.stations if s != self.passenger_loc]
        self.destination = random.choice(possible_destinations)
        
        return self.get_state(), {}

    def step(self, action):
        """æ›´æ–°ç¯å¢ƒçŠ¶æ€å¹¶è¿”å› (state, reward, done, info)"""
        old_state = self.get_state()
        old_pos = self.taxi_pos
        
        taxi_row, taxi_col = self.taxi_pos
        next_row, next_col = taxi_row, taxi_col
        reward = 0
        done = False

        # æ·»åŠ å½“å‰ä½ç½®åˆ°å†å²è®°å½•
        self.recent_positions.append(self.taxi_pos)
        if len(self.recent_positions) > self.position_history_limit:
            self.recent_positions.pop(0)
            
        # æ£€æµ‹å¾ªç¯è¡Œä¸º
        position_repeat_penalty = 0
        if len(self.recent_positions) > 5:
            position_counts = {}
            for pos in self.recent_positions:
                position_counts[pos] = position_counts.get(pos, 0) + 1
            # å¦‚æœæŸä¸ªä½ç½®é‡å¤è¶…è¿‡3æ¬¡ï¼Œç»™äºˆæƒ©ç½š
            for pos, count in position_counts.items():
                if count > 3:
                    position_repeat_penalty = -1.0 * count  # æƒ©ç½šä¸é‡å¤æ¬¡æ•°æˆæ¯”ä¾‹
            
        # æ ¹æ®åŠ¨ä½œæ›´æ–°ä½ç½®ï¼ˆç§»åŠ¨åŠ¨ä½œï¼‰
        if action == 0:  # Move Down
            next_row += 1
        elif action == 1:  # Move Up
            next_row -= 1
        elif action == 2:  # Move Right
            next_col += 1
        elif action == 3:  # Move Left
            next_col -= 1

        if action in [0, 1, 2, 3]:
            # è®¡ç®—è·ç¦»
            if self.passenger_picked_up:
                old_distance = abs(taxi_row - self.destination[0]) + abs(taxi_col - self.destination[1])
            else:
                old_distance = abs(taxi_row - self.passenger_loc[0]) + abs(taxi_col - self.passenger_loc[1])
        
            # å¤„ç†ç§»åŠ¨
            if (next_row, next_col) in self.obstacles or not (0 <= next_row < self.grid_size and 0 <= next_col < self.grid_size):
                reward -= 5
            else:
                self.taxi_pos = (next_row, next_col)
                if self.passenger_picked_up:
                    self.passenger_loc = self.taxi_pos
            
            # åŸºç¡€ç§»åŠ¨æƒ©ç½šï¼Œé™ä½ä¸º-0.05ï¼Œé¼“åŠ±æ›´å¤šæ¢ç´¢
            reward -= 0.05
            
            # æ›´åˆç†çš„shaping rewards
            if self.passenger_picked_up:
                reward += 0.2  # ä¹˜å®¢åœ¨è½¦ä¸Šæ—¶çš„å¥–åŠ±
                new_distance = abs(self.taxi_pos[0] - self.destination[0]) + abs(self.taxi_pos[1] - self.destination[1])
                # æ›´å¹³æ»‘çš„å¥–åŠ±æ¢¯åº¦
                if new_distance < old_distance:
                    reward += 2.0  # å‡å°‘æœç›®æ ‡ç§»åŠ¨çš„å¥–åŠ±ï¼Œè®©agentæ›´çµæ´»
                elif new_distance > old_distance:
                    reward -= 0.5  # å‡å°‘è¿œç¦»ç›®æ ‡çš„æƒ©ç½š
                
                # æ¥è¿‘ç›®çš„åœ°æ—¶çš„é¢å¤–å¥–åŠ±
                if new_distance == 1:  # è·ç¦»ç›®çš„åœ°ä»…1æ­¥
                    reward += 3.0
                elif new_distance == 0:  # åˆ°è¾¾ç›®çš„åœ°
                    reward += 10.0  # å¼ºçƒˆé¼“åŠ±åœ¨æœ‰ä¹˜å®¢æ—¶åˆ°è¾¾ç›®çš„åœ°
            else:
                new_distance = abs(self.taxi_pos[0] - self.passenger_loc[0]) + abs(self.taxi_pos[1] - self.passenger_loc[1])
                if new_distance < old_distance:
                    reward += 1.5
                elif new_distance > old_distance:
                    reward -= 0.4
                
                # æ¥è¿‘ä¹˜å®¢æ—¶çš„é¢å¤–å¥–åŠ±
                if new_distance == 1:  # è·ç¦»ä¹˜å®¢ä»…1æ­¥
                    reward += 2.0
                elif new_distance == 0:  # åˆ°è¾¾ä¹˜å®¢ä½ç½®
                    reward += 5.0  # å¼ºçƒˆé¼“åŠ±åˆ°è¾¾ä¹˜å®¢ä½ç½®
        else:
            # éç§»åŠ¨åŠ¨ä½œå¤„ç†
            if action == 4:  # PICKUP
                if self.passenger_picked_up:
                    reward -= 5  # å‡è½»é‡å¤æ¥å®¢çš„æƒ©ç½š
                elif self.taxi_pos == self.passenger_loc:
                    self.passenger_picked_up = True
                    self.passenger_loc = self.taxi_pos
                    reward += 30  # å¢åŠ æ¥å®¢å¥–åŠ±
                    self.successful_pickups += 1
                else:
                    reward -= 5  # å‡è½»é”™è¯¯æ¥å®¢çš„æƒ©ç½š
            elif action == 5:  # DROPOFF
               # åœ¨stepå‡½æ•¸ä¸­çš„DROPOFFå‹•ä½œéƒ¨åˆ†
                if self.passenger_picked_up:
                    if self.taxi_pos == self.destination:
                        reward += 500  # æé«˜å¾100åˆ°500ï¼Œä½¿å…¶é é«˜æ–¼å…¶ä»–çå‹µ
                        done = True
                        self.successful_dropoffs += 1
                    else:
                        reward -= 30  # æ¸›è¼•å°éŒ¯èª¤åœ°é»ä¸‹å®¢çš„æ‡²ç½°
        # åŸºç¡€æ“ä½œæƒ©ç½š
        reward -= 0.05

        # æ‰£é™¤ç‡ƒæ–™
        self.current_fuel -= 1
        if self.current_fuel <= 0:
            reward -= 10
            done = True

        # æ£€æŸ¥æ–°æ—§çŠ¶æ€æ˜¯å¦ç›¸åŒ
        new_state = self.get_state()
        if old_state == new_state and old_pos == self.taxi_pos:
            reward -= 1  # å‡è½»çŠ¶æ€æœªå˜çš„æƒ©ç½š

        # æ›´æ–°å¹¶æ£€æŸ¥æœ€è¿‘è¡ŒåŠ¨
        self.action_history.append(action)
        if len(self.action_history) > 8:
            self.action_history.pop(0)
            
        # åº”ç”¨å¾ªç¯ä½ç½®æƒ©ç½š
        reward += position_repeat_penalty

         # å¦‚æœä¹˜å®¢åœ¨è½¦ä¸Šï¼Œæ¯ä¸€æ­¥é¢å¤–å¥–åŠ± +0.1
        if self.passenger_picked_up:
            reward += 0.2
        return new_state, reward, done, {}

    # def get_state(self):
    #     """è¿”å›å½“å‰ç¯å¢ƒçŠ¶æ€ (tuple)"""
    #     taxi_row, taxi_col = self.taxi_pos
    #     passenger_row, passenger_col = self.passenger_loc
    #     destination_row, destination_col = self.destination
        
    #     # æ£€æŸ¥å‘¨å›´éšœç¢ç‰©
    #     obstacle_north = int(taxi_row == 0 or (taxi_row-1, taxi_col) in self.obstacles)
    #     obstacle_south = int(taxi_row == self.grid_size - 1 or (taxi_row+1, taxi_col) in self.obstacles)
    #     obstacle_east  = int(taxi_col == self.grid_size - 1 or (taxi_row, taxi_col+1) in self.obstacles)
    #     obstacle_west  = int(taxi_col == 0 or (taxi_row, taxi_col-1) in self.obstacles)
        
    #     # æ£€æŸ¥ä¹˜å®¢ä½ç½®ç›¸å¯¹äºTaxiçš„ä½ç½®
    #     passenger_loc_north = int((taxi_row - 1, taxi_col) == self.passenger_loc)
    #     passenger_loc_south = int((taxi_row + 1, taxi_col) == self.passenger_loc)
    #     passenger_loc_east  = int((taxi_row, taxi_col + 1) == self.passenger_loc)
    #     passenger_loc_west  = int((taxi_row, taxi_col - 1) == self.passenger_loc)
    #     passenger_loc_middle = int((taxi_row, taxi_col) == self.passenger_loc)
    #     passenger_look = (passenger_loc_north or passenger_loc_south or 
    #                       passenger_loc_east or passenger_loc_west or passenger_loc_middle)
        
    #     # æ£€æŸ¥ç›®çš„åœ°ä½ç½®ç›¸å¯¹äºTaxiçš„ä½ç½®
    #     destination_loc_north = int((taxi_row - 1, taxi_col) == self.destination)
    #     destination_loc_south = int((taxi_row + 1, taxi_col) == self.destination)
    #     destination_loc_east  = int((taxi_row, taxi_col + 1) == self.destination)
    #     destination_loc_west  = int((taxi_row, taxi_col - 1) == self.destination)
    #     destination_loc_middle = int((taxi_row, taxi_col) == self.destination)
    #     destination_look = (destination_loc_north or destination_loc_south or 
    #                         destination_loc_east or destination_loc_west or destination_loc_middle)
        
    #     # çŠ¶æ€ä¿¡æ¯ï¼šæ·»åŠ ä¹˜å®¢æ˜¯å¦åœ¨è½¦ä¸Šä½œä¸ºçŠ¶æ€çš„ä¸€éƒ¨åˆ†
    #     state = (taxi_row, taxi_col,
    #              self.stations[0][0], self.stations[0][1],
    #              self.stations[1][0], self.stations[1][1],
    #              self.stations[2][0], self.stations[2][1],
    #              self.stations[3][0], self.stations[3][1],
    #              obstacle_north, obstacle_south, obstacle_east, obstacle_west,
    #              passenger_look, destination_look, int(self.passenger_picked_up))  # æ·»åŠ ä¹˜å®¢æ˜¯å¦åœ¨è½¦ä¸Š
    #     return state
    def get_state(self):
        """è¿”å›ç•¶å‰ç’°å¢ƒç‹€æ…‹ (tuple)ï¼Œå¢åŠ äº†taxièˆ‡passengerå’Œdestinationçš„ç›´æ¥è·é›¢"""
        taxi_row, taxi_col = self.taxi_pos
        
        # è¨ˆç®— taxi åˆ°å››å€‹ç«™é»çš„æ›¼å“ˆé “è·é›¢
        distances_to_stations = [abs(taxi_row - station[0]) + abs(taxi_col - station[1]) for station in self.stations]
        
        # è¨ˆç®— taxi åˆ°ä¹˜å®¢çš„æ›¼å“ˆé “è·é›¢
        distance_to_passenger = abs(taxi_row - self.passenger_loc[0]) + abs(taxi_col - self.passenger_loc[1])
        
        # è¨ˆç®— taxi åˆ°ç›®çš„åœ°çš„æ›¼å“ˆé “è·é›¢
        distance_to_destination = abs(taxi_row - self.destination[0]) + abs(taxi_col - self.destination[1])
        
        # éšœç¤™ç‰©ä¿¡æ¯ï¼šæª¢æ¸¬ taxi å››å€‹æ–¹å‘æ˜¯å¦å­˜åœ¨éšœç¤™ç‰©æˆ–è¶Šç•Œ
        obstacle_north = int(taxi_row == 0 or (taxi_row - 1, taxi_col) in self.obstacles)
        obstacle_south = int(taxi_row == self.grid_size - 1 or (taxi_row + 1, taxi_col) in self.obstacles)
        obstacle_east = int(taxi_col == self.grid_size - 1 or (taxi_row, taxi_col + 1) in self.obstacles)
        obstacle_west = int(taxi_col == 0 or (taxi_row, taxi_col - 1) in self.obstacles)
        obstacles = [obstacle_north, obstacle_south, obstacle_east, obstacle_west]
        
        # ä¹˜å®¢æ˜¯å¦åœ¨è»Šä¸Š
        passenger_in_taxi = int(self.passenger_picked_up)
        
        # ä¹˜å®¢å’Œç›®çš„åœ°èˆ‡Taxiçš„ç›¸å°ä½ç½®æ¨™èªŒï¼ˆæ˜¯å¦åœ¨Taxiçš„ç›¸é„°ä½ç½®ï¼‰
        passenger_adjacent = int(distance_to_passenger <= 1) 
        destination_adjacent = int(distance_to_destination <= 1)
        
        # çµ„åˆæ‰€æœ‰ç‰¹å¾µæˆä¸€å€‹tuple
        state = tuple(distances_to_stations + obstacles + 
                    [passenger_in_taxi, distance_to_passenger, distance_to_destination,
                    passenger_adjacent, destination_adjacent])
        return state


    def render_env(self, taxi_pos, action=None, step=None, fuel=None):
        clear_output(wait=True)
        grid = [['.'] * self.grid_size for _ in range(self.grid_size)]
        
        # è®¾ç½®å››ä¸ªç«™ç‚¹ (ä¾‹å¦‚ï¼šR, G, Y, B)
        grid[0][0] = 'R'
        grid[0][self.grid_size - 1] = 'G'
        grid[self.grid_size - 1][0] = 'Y'
        grid[self.grid_size - 1][self.grid_size - 1] = 'B'
        
        # æ˜¾ç¤º Taxi
        ty, tx = taxi_pos
        if 0 <= tx < self.grid_size and 0 <= ty < self.grid_size:
            grid[ty][tx] = 'ğŸš–'
            
        print(f"\nStep: {step}")
        print(f"Taxi Position: ({tx}, {ty})")
        print(f"Fuel Left: {fuel}")
        print(f"Last Action: {self.get_action_name(action)}")
        print(f"Passenger Picked Up: {self.passenger_picked_up}")
        print(f"Passenger Location: {self.passenger_loc}")
        print(f"Destination: {self.destination}\n")
        
        for row in grid:
            print(" ".join(row))
        print("\n")

    def get_action_name(self, action):
        actions = ["Move South", "Move North", "Move East", "Move West", "Pick Up", "Drop Off"]
        return actions[action] if action is not None else "None"

class DuelingDQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DuelingDQN, self).__init__()
        
        # ç‰¹å¾æå–å±‚
        self.feature_layer = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        # ä»·å€¼æµ
        self.value_stream = nn.Sequential(
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        # ä¼˜åŠ¿æµ
        self.advantage_stream = nn.Sequential(
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )
    
    def forward(self, state):
        features = self.feature_layer(state)
        values = self.value_stream(features)
        advantages = self.advantage_stream(features)
        # è®¡ç®—Qå€¼: Q(s,a) = V(s) + (A(s,a) - mean(A(s,:)))
        qvals = values + (advantages - advantages.mean(dim=1, keepdim=True))
        return qvals

class DQNAgent:
    def __init__(self, state_dim, action_dim, device, lr=3e-4, gamma=0.99, 
                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.998):
        self.device = device
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        
        # Epsilonç­–ç•¥å‚æ•°
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        
        # ä½¿ç”¨Dueling DQNç½‘ç»œ
        self.policy_net = DuelingDQN(state_dim, action_dim).to(device)
        self.target_net = DuelingDQN(state_dim, action_dim).to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()
        
        # ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ç¨ä½ä¸€äº›
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)
        # self.optimizer = optim.SGD(self.policy_net.parameters(), lr=lr, momentum=0.9)
        
        # æ›´å¤§çš„å›æ”¾ç¼“å†²åŒº
        self.memory = ReplayMemory(10000)
        self.batch_size = 64
        
        # æ¢ç´¢å¥–åŠ±ç›¸å…³
        self.state_counts = {}
        self.explore_coef = 0.5  # å‡å°æ¢ç´¢å¥–åŠ±ç³»æ•°
        
        # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=200, gamma=0.5)
        
    def select_action(self, state):
        # Epsilon-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        if random.random() < self.epsilon:
            action = random.randrange(self.action_dim)
        else:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.policy_net(state_tensor)
                action = q_values.max(1)[1].item()
        
        # Epsiloné€’å‡
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)
        return action
    
    def remember(self, state, action, reward, next_state, done):
        self.memory.push(state, action, reward, next_state, done)
    
    def optimize_model(self):
        if len(self.memory) < self.batch_size:
            return
        
        transitions = self.memory.sample(self.batch_size)
        batch = Transition(*zip(*transitions))
        
        state_batch = torch.FloatTensor(batch.state).to(self.device)
        action_batch = torch.LongTensor(batch.action).unsqueeze(1).to(self.device)
        reward_batch = torch.FloatTensor(batch.reward).to(self.device)
        next_state_batch = torch.FloatTensor(batch.next_state).to(self.device)
        done_batch = torch.FloatTensor(batch.done).to(self.device)
        
        # è®¡ç®—å½“å‰Qå€¼
        q_values = self.policy_net(state_batch).gather(1, action_batch).squeeze(1)
        
        # Double DQN: ä½¿ç”¨policy_neté€‰æ‹©actionï¼Œä½¿ç”¨target_netè¯„ä¼°
        with torch.no_grad():
            next_actions = self.policy_net(next_state_batch).max(1)[1].unsqueeze(1)
            next_q_values = self.target_net(next_state_batch).gather(1, next_actions).squeeze(1)
            expected_q_values = reward_batch + self.gamma * next_q_values * (1 - done_batch)
        
        # ä½¿ç”¨Huber Lossï¼Œå¯¹äºå¼‚å¸¸å€¼æ›´é²æ£’
        loss = F.smooth_l1_loss(q_values, expected_q_values)
        
        # æ¢¯åº¦ä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)
        self.optimizer.step()
        
        return loss.item()
    
    def update_target_network(self):
        """ä½¿ç”¨è½¯æ›´æ–°ç­–ç•¥æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        tau = 0.01  # è½¯æ›´æ–°ç³»æ•°
        for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):
            target_param.data.copy_(tau * policy_param.data + (1 - tau) * target_param.data)
    
    def get_exploration_bonus(self, state):
        """è®¡ç®—æ¢ç´¢å¥–åŠ±"""
        state_key = tuple(state)
        self.state_counts[state_key] = self.state_counts.get(state_key, 0) + 1
        bonus = self.explore_coef / math.sqrt(self.state_counts[state_key])
        return bonus
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'policy_net': self.policy_net.state_dict(),
            'target_net': self.target_net.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'epsilon': self.epsilon
        }, path)
    
    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path)
        self.policy_net.load_state_dict(checkpoint['policy_net'])
        self.target_net.load_state_dict(checkpoint['target_net'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.epsilon = checkpoint['epsilon']

def process_state_for_network(state):
    """
    è™•ç† get_state() è¿”å›çš„ç‹€æ…‹ï¼Œç¾åœ¨ç‹€æ…‹åŒ…å«ï¼š
      - åˆ°4å€‹ç«™é»çš„è·é›¢ (4å€‹å€¼)
      - éšœç¤™ç‰©ä¿¡æ¯ (4å€‹å€¼)
      - ä¹˜å®¢æ˜¯å¦åœ¨è»Šä¸Š (1å€‹å€¼)
      - åˆ°ä¹˜å®¢çš„è·é›¢ (1å€‹å€¼)
      - åˆ°ç›®çš„åœ°çš„è·é›¢ (1å€‹å€¼)
      - ä¹˜å®¢æ˜¯å¦åœ¨ç›¸é„°ä½ç½® (1å€‹å€¼)
      - ç›®çš„åœ°æ˜¯å¦åœ¨ç›¸é„°ä½ç½® (1å€‹å€¼)
    """
    # å‡è¨­æœ€å¤§æ›¼å“ˆé “è·é›¢ç‚ºç’°å¢ƒå¤§å°çš„2å€
    max_distance = 10.0  # å°æ–¼5x5ç’°å¢ƒï¼Œæœ€å¤§è·é›¢æ˜¯8ï¼Œç¨å¾®æ”¾å¯¬ä¸€é»
    
    # æ­¸ä¸€åŒ–åˆ°ç«™é»çš„è·é›¢
    distances_to_stations = [s / max_distance for s in state[:4]]
    
    # éšœç¤™ç‰©ä¿¡æ¯ï¼Œä¸éœ€è¦æ­¸ä¸€åŒ–
    obstacles = list(state[4:8])
    
    # ä¹˜å®¢æ˜¯å¦åœ¨è»Šä¸Š
    passenger_in_taxi = [state[8]]
    
    # æ­¸ä¸€åŒ–åˆ°ä¹˜å®¢å’Œç›®çš„åœ°çš„è·é›¢
    distance_to_passenger = [state[9] / max_distance]
    distance_to_destination = [state[10] / max_distance]
    
    # ä¹˜å®¢å’Œç›®çš„åœ°æ˜¯å¦åœ¨ç›¸é„°ä½ç½®
    passenger_adjacent = [state[11]]
    destination_adjacent = [state[12]]
    
    # åˆä½µæ‰€æœ‰ç‰¹å¾µ
    processed_state = (distances_to_stations + obstacles + passenger_in_taxi + 
                       distance_to_passenger + distance_to_destination + 
                       passenger_adjacent + destination_adjacent)
    
    return processed_state

def train_dqn(env, agent, num_episodes=1000, save_interval=200, render_interval=100):
    """è®­ç»ƒDQNä»£ç†"""
    total_rewards = []
    avg_rewards = []  # ä¿å­˜å¹³å‡å¥–åŠ±
    best_avg_reward = -float('inf')
    episode_lengths = []
    
    # æ¯ä¸ªepisodeçš„ç»Ÿè®¡æ•°æ®
    pickup_success_rate = []
    dropoff_success_rate = []
    
    # æ¯render_intervalæ¬¡è¯„ä¼°ä¸€ä¸‹æ¨¡å‹
    evaluation_rewards = []
    
    for episode in range(num_episodes):
        state, _ = env.reset()
        processed_state = process_state_for_network(state)
        episode_reward = 0
        done = False
        step_count = 0
        
        pickup_attempted = False
        dropoff_attempted = False
        
        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.select_action(processed_state)
            
            # è®°å½•å°è¯•çš„æ¥å®¢å’Œé€å®¢
            if action == 4:  # PICKUP
                pickup_attempted = True
            elif action == 5:  # DROPOFF
                dropoff_attempted = True
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, _ = env.step(action)
            processed_next_state = process_state_for_network(next_state)
            
            # è®¡ç®—æ¢ç´¢å¥–åŠ±ï¼ˆå¯é€‰ï¼‰
            if episode < num_episodes // 2:  # åªåœ¨å‰åŠéƒ¨åˆ†è®­ç»ƒä¸­ä½¿ç”¨æ¢ç´¢å¥–åŠ±
                bonus = agent.get_exploration_bonus(processed_next_state)
                total_reward = reward + bonus
            else:
                total_reward = reward
            
            # å­˜å‚¨ç»éªŒ
            agent.remember(processed_state, action, total_reward, processed_next_state, done)
            
            # ä¼˜åŒ–æ¨¡å‹
            loss = agent.optimize_model()
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            agent.update_target_network()
            
            # æ›´æ–°çŠ¶æ€
            processed_state = processed_next_state
            episode_reward += reward
            step_count += 1
            
            # é˜²æ­¢è¿‡é•¿çš„episode
            if step_count >= 100:
                done = True
        
        # è®°å½•ç»Ÿè®¡æ•°æ®
        total_rewards.append(episode_reward)
        episode_lengths.append(step_count)
        
        # è®¡ç®—æœ€è¿‘100ä¸ªepisodeçš„å¹³å‡å¥–åŠ±
        if len(total_rewards) >= 100:
            avg_reward = np.mean(total_rewards[-100:])
            avg_rewards.append(avg_reward)
            
            # å¦‚æœå¹³å‡å¥–åŠ±æé«˜ï¼Œä¿å­˜æ¨¡å‹
            if avg_reward > best_avg_reward:
                best_avg_reward = avg_reward
                agent.save_model("best_taxi_model.pth")
        else:
            avg_rewards.append(np.mean(total_rewards))
        
        # è®¡ç®—æ¥å®¢å’Œé€å®¢æˆåŠŸç‡
        pickup_success = env.successful_pickups / max(1, episode + 1)
        dropoff_success = env.successful_dropoffs / max(1, episode + 1)
        pickup_success_rate.append(pickup_success)
        dropoff_success_rate.append(dropoff_success)
        
        # æŒ‰æŒ‡å®šé—´éš”æ¸²æŸ“ç¯å¢ƒå’Œæ‰“å°ç»Ÿè®¡ä¿¡æ¯
        if episode % 10 == 0:
            print(f"Episode {episode+1}/{num_episodes}, Steps: {step_count}, "
                  f"Reward: {episode_reward:.2f}, Avg Reward: {avg_rewards[-1]:.2f}, "
                  f"Epsilon: {agent.epsilon:.4f}, Loss: {loss if loss else 'N/A'}")
            print(f"Successful Pickups: {env.successful_pickups}, "
                  f"Successful Dropoffs: {env.successful_dropoffs}, "
                  f"Pickup Success Rate: {pickup_success:.4f}, "
                  f"Dropoff Success Rate: {dropoff_success:.4f}")
        
        # å®šæœŸä¿å­˜æ¨¡å‹
        if episode % save_interval == 0 and episode > 0:
            agent.save_model(f"taxi_model_episode_{episode}.pth")
    
    # è¿”å›è®­ç»ƒæ•°æ®
    return {
        'rewards': total_rewards,
        'avg_rewards': avg_rewards,
        'episode_lengths': episode_lengths,
        'pickup_success_rate': pickup_success_rate,
        'dropoff_success_rate': dropoff_success_rate
    }

# def student_agent_get_action(obs):
#     """
#     ç”¨äºæœ€ç»ˆæäº¤çš„å‡½æ•°ï¼Œä»ç¯å¢ƒè§‚å¯Ÿè¿”å›è¡ŒåŠ¨
#     """
#     # 1. åŠ è½½æ¨¡å‹
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
#     # åˆ›å»ºä¸€ä¸ªä¸´æ—¶agentå¯¹è±¡ï¼Œä»…ç”¨äºåŠ è½½æ¨¡å‹
#     state_dim = len(process_state_for_network(obs))
#     action_dim = 6
#     agent = DQNAgent(state_dim, action_dim, device)
    
#     # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
#     agent.load_model("best_taxi_model.pth")
    
#     # 2. å¤„ç†è§‚å¯Ÿ
#     processed_obs = process_state_for_network(obs)
    
#     # 3. é€‰æ‹©åŠ¨ä½œï¼ˆæµ‹è¯•æ¨¡å¼ï¼Œä¸éœ€è¦æ¢ç´¢ï¼‰
#     with torch.no_grad():
#         state_tensor = torch.FloatTensor(processed_obs).unsqueeze(0).to(device)
#         q_values = agent.policy_net(state_tensor)
#         action = q_values.max(1)[1].item()
    
#     return action
# å…¨å±€è®Šé‡å­˜å„²æ¨¡å‹
_model = None
_device = None
def get_action(obs):
    """
    è©•ä¼°å‡½æ•¸ï¼šæ¥æ”¶ç’°å¢ƒè§€å¯Ÿä¸¦è¿”å›å‹•ä½œ
    
    é€™æ˜¯ä½œæ¥­è¦æ±‚çš„ä¸»è¦å‡½æ•¸ï¼Œè©•ä¼°ç³»çµ±æœƒèª¿ç”¨æ­¤å‡½æ•¸
    """
    global _model, _device
    
    # é¦–æ¬¡èª¿ç”¨æ™‚åŠ è¼‰æ¨¡å‹
    if _model is None:
        # ç¢ºå®šè¨­å‚™
        _device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è¨ˆç®—ç‹€æ…‹ç¶­åº¦
        processed_obs = process_state_for_network(obs)
        state_dim = len(processed_obs)
        action_dim = 6
        
        # å‰µå»ºæ¨¡å‹
        _model = DuelingDQN(state_dim, action_dim).to(_device)
        
        # å˜—è©¦åŠ è¼‰æ¨¡å‹ï¼Œå„ªå…ˆå˜—è©¦ä¸åŒå¯èƒ½çš„è·¯å¾‘
        model_paths = [
            "best_taxi_model.pth",
            "final_taxi_model.pth",
            os.path.join(os.path.dirname(__file__), "best_taxi_model.pth"),
            os.path.join(os.path.dirname(__file__), "final_taxi_model.pth")
        ]
        
        for path in model_paths:
            try:
                checkpoint = torch.load(path, map_location=_device, weights_only=True)
                _model.load_state_dict(checkpoint['policy_net'])
                _model.eval()  # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
                print(f"æˆåŠŸåŠ è¼‰æ¨¡å‹: {path}")
                break
            except Exception as e:
                continue
    
    # è™•ç†è§€å¯Ÿä¸¦é¸æ“‡å‹•ä½œ
    processed_obs = process_state_for_network(obs)
    
    with torch.no_grad():
        state_tensor = torch.FloatTensor(processed_obs).unsqueeze(0).to(_device)
        q_values = _model(state_tensor)
        action = q_values.max(1)[1].item()
    
    return action
# ä¸»å‡½æ•°
if __name__ == "__main__":
    # ç¯å¢ƒé…ç½®
    env_config = {
        "grid_size": 5,
        "fuel_limit": 1000
    }
    env = SimpleTaxiEnv(**env_config)
    
    # è·å–çŠ¶æ€ç»´åº¦
    sample_state, _ = env.reset()
    processed_sample = process_state_for_network(sample_state)
    state_dim = len(processed_sample)
    action_dim = 6
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # åˆ›å»ºDQNä»£ç†
    agent = DQNAgent(state_dim, action_dim, device, lr=5e-4, gamma=0.99,
                 epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.99999)  # åŠ å¿«epsilonè¡°æ¸›
    
    # è®­ç»ƒä»£ç†
    train_results = train_dqn(env, agent, num_episodes=2000, save_interval=200)
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    print("è®­ç»ƒç»“æŸï¼")
    print(f"Total Successful Pickups: {env.successful_pickups}")
    print(f"Total Successful Dropoffs: {env.successful_dropoffs}")
    print(f"Final Pickup Success Rate: {env.successful_pickups/2000:.4f}")
    print(f"Final Dropoff Success Rate: {env.successful_dropoffs/2000:.4f}")
    
    # æ˜¾ç¤ºæ¨¡å‹å‚æ•°æ•°é‡
    num_params = sum(p.numel() for p in agent.policy_net.parameters() if p.requires_grad)
    print(f"Number of trainable parameters in the DQN: {num_params}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    agent.save_model("final_taxi_model.pth")